# Generating financial senarios with a variational autoencoder to compute risk metrics

## What is a Variational Autoencoder?

A **VAE** is a probabilistic version of the traditional autoencoder. Instead of mapping inputs to fixed points in latent space, it maps them to a distribution — typically a Gaussian — allowing for sampling and smooth interpolation between data points.

The key components are:

- **Encoder**: Compresses the input into a mean and standard deviation
- **Latent Space Sampling**: Samples from the latent distribution using the reparameterization trick
- **Decoder**: Reconstructs the input from the latent representation

![image](https://github.com/user-attachments/assets/09cba6bc-19e1-4d21-a38e-705de58d0fc1)

## Output of the code

This code will take as an input 2 csv files containings market data of some indexes and interest rates, a XML file containing the instruments we want to price.

It will output the Value at risk of the portolio with a confidence level of 95% horizon 5 days. The VAR will be computed by the 95 quartile of the simulations generated by the VAE.

![image](https://github.com/user-attachments/assets/4aa89310-e98b-4ab2-9dc8-c2228b4e9dc7)

## Theory behind the VAE

We have 3 distribution :
* $P(Z)$ **"Prior"** that is a normal law: $\mathcal{N}(0,1)$ and it is the distribution that we want to obtain in the latent space.
* $P(Z|X)$ **"Posterior"** "probability of obtaining Z from X. Probability for the encoder. It will be approximated by a gaussian distribution : $Q(Z|X)$ because $P(Z|X)$ is not known.
* $P(X|Z)$ **"Reconstruction"** "probability of obtaining X by decoding Z", Probability of the decoder. 

What we want to compute is : 

$$P(Z|X) = \frac{P(Z,X)}{P(X)} = \frac{P(X|Z)P(Z)}{\int_ZP(X,z)dz}$$

but $\int_ZP(X,z)dz$ is not tractable, because we will need to compute it for every value of $Z$, the latter can be highly dimentional. 

So the solution is to approximate the "posterior" with another distribution : $Q(Z)$.

To mesure the error that we will make from this approximation we will use the Kulback Leibler divergence : 

$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z|X)}$$

It tells us the amount of information in bits needed to distore $Q$ to $P$.

$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)P(X)}{P(z,X)} = \sum_{z \in Z} Q(z|X)\left[log\frac{Q(z|X)}{P(z,X)} + log P(X)\right] $$

$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} + \sum_{z \in Z} Q(z|X)log P(X)$$


$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} + log P(X)\sum_{z \in Z} Q(z|X)$$


$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} + log P(X)$$

On the equation above, the first term is the term that we will want to minimize. So let's focus on this first term :

$$\sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} = \mathbb{E}_{Q(Z|x)}\left[log\frac{Q(Z|x)}{P(Z,x)}\right]$$

$$\mathbb{E}_{Q(Z|X)} \left[ \log \frac{Q(Z|X)}{P(X, S)} \right]  $$ 

$$ \mathbb{E}_{Q(Z|X)} \left[ \log Q(Z|X) - \log P(X|Z) - \log P(Z) \right]$$


We want to minimize this term, so maximize the opposit :

$$\text{maximize} \mathcal{L} = - \sum_Z Q(Z|x)log\frac{Q(Z|x)}{P(Z,x)} = \mathbb{E}_{Q(Z|x)}\left[log P(x|Z) + log\frac{P(Z)}{Q(Z|X)}\right]$$


$\mathcal{L}$ is the variational lower bond.

Moreover : 

$$\mathcal{L} = \mathbb{E}_{Q(Z|X)} \left[log P(X|Z) + log\frac{P(Z)}{Q(Z|X)}\right] $$

$$\mathcal{L} = \mathbb{E}_{Q(Z|X)}\left[log P(X|Z)\right] + \sum_Z Q(Z|X) log \frac{P(Z)}{Q(Z|X)}$$




$$\mathcal{L} = \mathbb{E}_{Q(Z|x)}\left[log P(x|Z) \right] - KL\left(Q(Z|X)||P(Z)\right)$$

With what we had earlier we then have : 

$$KL(Q(Z|x)||P(Z|X)) = log P(X) - \mathcal{L} = log P(X) - \left(\mathbb{E}_{Q(Z|X)} \left[log P(X|Z) - KL(Q(Z|X)||P(Z)) \right]\right)$$

So :

$$log P(X) = \mathcal{L} + KL(Q(Z|X)||P(Z|x))$$

We want to maximize $log P(X)$, which is the log likelihood of the data. And $\mathcal{L}$ is the lowerbond of $logP(X)$. If $\mathcal{L}$ increases, the better you describe your data, because $P(X)$ increases. So we want to maximize the $\mathcal{L}$ term.

$$\mathcal{L} = \mathbb{E}_{Q(Z|X)} \left[log P(X|Z) \right] - KL (Q(Z|X)||P(Z))$$

The first term is negative, it reprsents the reconstruction error in our case it will be the MSE. The second term is the regularization term.


$$KL(Q(Z|X)||\mathcal{N}(0,1)) = - \frac{1}{2}\left[-\sigma_q^2 - \mu_q^2 + 1 + log(\sigma_q^2)\right]$$

Reparametrisation trick :
In the back propagation process, we need to compute a gradient, thus diffecienting a function.
How to differenciate Z, which has been generated with $\mathcal{N}(\mu_q,\sigma_q^2)$ ?

The trick is the generate Z $\mu_q + \sigma_q^2 \epsilon$ where $\epsilon$ is a nomal law $\mathcal{N}(0,1)$. In this way Z becomes differenciable


