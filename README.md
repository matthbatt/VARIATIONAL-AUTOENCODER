# Generating financial senarios with a variational autoencoder to compute risk metrics

This project implements a **Variational Autoencoder (VAE)** — a type of generative model and We will use this VAE to generate financial senarios to compute risk metrics like the Value at risk


## What is a Variational Autoencoder?

A **VAE** is a probabilistic version of the traditional autoencoder. Instead of mapping inputs to fixed points in latent space, it maps them to a distribution — typically a Gaussian — allowing for sampling and smooth interpolation between data points.

The key components are:

- **Encoder**: Compresses the input into a mean and standard deviation
- **Latent Space Sampling**: Samples from the latent distribution using the reparameterization trick
- **Decoder**: Reconstructs the input from the latent representation

![image](https://github.com/user-attachments/assets/09cba6bc-19e1-4d21-a38e-705de58d0fc1)

## Output of the code

This code will take as an input 2 csv files containings market data of some indexes and interest rates. And a XML file containing the instruments we want to price.

It will output the Value at risk of the portolio with a confidence level of 95% horizon 5 days. The VAR will be computed by the 95 quartile of the simulations generated by the VAE.

![image](https://github.com/user-attachments/assets/4aa89310-e98b-4ab2-9dc8-c2228b4e9dc7)

## Theory behind the VAE

We have 3 distribution :
* $P(Z)$ **"Prior"** that is a normal law: $\mathcal{N}(0,1)$ and it is the distribution that we want to obtain in the latent space.
* $P(Z|X)$ **"Posterior"** "probability of obtaining Z from X. Probability for the encoder. It will be approximated by a gaussian distribution : $Q(Z|X)$ because $P(Z|X)$ is not known.
* $P(X|Z)$ **"Reconstruction"** "probability of obtaining X by decoding Z", Probability of the decoder. 

What we want to compute is : 

$$P(Z|X) = \frac{P(Z,X)}{P(X)} = \frac{P(X|Z)P(Z)}{\int_ZP(X,z)dz}$$

but $\int_ZP(X,z)dz$ is not tractable, because we will need to compute it for every value of $Z$, the latter can be highly dimentional. 

So the solution is to approximate the "posterior" with another distribution : $Q(Z)$.

To mesure the error that we will make from this approximation we will use the Kulback Leibler divergence : 

$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z|X)}$$

It tells us the amount of information in bits needed to distore $Q$ to $P$.

$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)P(X)}{P(z,X)} = \sum_{z \in Z} Q(z|X)\left[log\frac{Q(z|X)}{P(z,X)} + log P(X)\right] $$

$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} + \sum_{z \in Z} Q(z|X)log P(X)$$


$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} + log P(X)\sum_{z \in Z} Q(z|X)$$


$$KL(Q(Z|X) | P(Z|X) ) = \sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} + log P(X)$$

On the equation above, the first term is the term that we will want to minimize. So let's focus on this first term :

$$\sum_{z \in Z} Q(z|X)log\frac{Q(z|X)}{P(z,X)} = \mathbb{E}_{Q(Z|x)}\left[log\frac{Q(Z|x)}{P(Z,x)}\right]$$

$$
\mathbb{E}_{Q(Z|X)}\left[ \log \frac{Q(Z|X)}{P(X, S)} \right] 
= \mathbb{E}_{Q(Z|X)}\left[ \log Q(Z|X) - \log P(X|Z) - \log P(Z) \right]
$$


We want to minimize this term, so maximize the opposit :

$$\text{maximize} \mathcal{L} = - \sum_Z Q(Z|x)log\frac{Q(Z|x)}{P(Z,x)} = \mathbb{E}_{Q(Z|x)}\left[log P(x|Z) + log\frac{P(Z)}{Q(Z|X)}\right]$$


$\mathcal{L}$ is the variational lower bond.

Moreover : 

$$\mathcal{L} = \mathbb{E}_{Q(Z|X)} \left[log P(X|Z) + log\frac{P(Z)}{Q(Z|X)}\right]$$

$$
\mathcal{L} = \mathbb{E}_{Q(Z|X)} \left[ log P(X|Z) + log \frac{P(Z)}{Q(Z|X)} \right] 
= \mathbb{E}_{Q(Z|X)}\left[log P(X|Z)\right] + \sum_Z Q(Z|X) log \frac{P(Z)}{Q(Z|X)}
$$


$$\mathcal{L} = \mathbb{E}_{Q(Z|x)}\left[log P(x|Z) \right] - KL\left(Q(Z|X)||P(Z)\right)$$


By computation we get :

$$log(P(X)) = \mathcal{L} + KL(Q(Z|X)||P(Z|X))$$

With $\mathcal{L}$ that is the **variational lower bound**, and it's opposit : $-\mathcal{L}$ that is our loss function.

We want to maximize $log(P(X))$ that is negative, so maximize $\mathcal{L}$(négatif) thus minizing $-\mathcal{L}$(positif). More precisely, minimizing :

\\
$$- \mathcal{L} = - \mathbb{E}_{Q(Z|X)}[log(P(X|Z)] + KL(Q(Z|X)||P(Z))$$

\
* $\mathbb{E}_{Q(Z|X)}[log(P(X|Z)]$ : is the reconsrtuction error, in our case it is the MSE (Mean Squarred Error) :
\\
* $KL(Q(Z|X)||P(Z))$ : is the regularization, the KL loss (Kulback Leibler). In our case since $P(Z)$ is a normal law, the KL loss can be written as :
$$KL(Q(Z|X)||\mathcal{N}(0,1)) = - \frac{1}{2}\left[-\sigma_q^2 - \mu_q^2 + 1 + log(\sigma_q^2)\right]$$

Reparametrisation trick :
In the back propagation process, we need to compute a gradient, thus diffecienting a function.
How to differenciate Z, which has been generated with $\mathcal{N}(\mu_q,\sigma_q^2)$ ?

The trick is the generate Z $\mu_q + \sigma_q^2 \epsilon$ where $\epsilon$ is a nomal law $\mathcal{N}(0,1)$. In this way Z becomes differenciable


